{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bd439d-b595-4678-85be-913ef3835faa",
   "metadata": {},
   "source": [
    "# Q1. Explain some methods to handle missing values in that data.\n",
    "Some of the methods to handle missing values are as follows:\n",
    "\n",
    "- Removing the rows and columns with null values if it has very less valuable information.\n",
    "- Imputing null values with descriptive statistical measures like mean, mode, and median.\n",
    "- Using methods like Simple Imputer or KNN Imputer to impute the null values in a more sophisticated way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cdc40-e5dd-46de-ae61-5e51e5b64990",
   "metadata": {},
   "source": [
    "# Q2. Why do we perform normalization?\n",
    "To achieve stable and fast training of the model we use normalization techniques to bring all the features to a certain scale or range of values. If we do not perform normalization then there are chances that the gradient will not converge to the global or local minima and end up oscillating back and forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00970c-cfed-4f31-a23d-1bf4a8d872ed",
   "metadata": {},
   "source": [
    "# Q3. What is the difference between covariance and correlation?\n",
    "As the name suggests, Covariance provides us with a measure of the extent to which two variables differ from each other. But on the other hand, correlation gives us the measure of the extent to which the two variables are related to each other. Covariance can take on any value while correlation is always between -1 and 1. These measures are used during the exploratory data analysis to gain insights from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9ab3d-8dcd-4218-8ebe-6514a7223aad",
   "metadata": {},
   "source": [
    "# Q4. What is KNN Imputer?\n",
    "We generally impute null values by the descriptive statistical measures of the data like mean, mode, or median but KNN Imputer is a more sophisticated method to fill the null values. A distance parameter is also used in this method which is also known as the k parameter. The work is somehow similar to the clustering algorithm. The missing value is imputed in reference to the neighborhood points of the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a3098-ca21-4da6-b399-a40259978e0d",
   "metadata": {},
   "source": [
    "# Q5. What is feature engineering? How does it affect the model’s performance? \n",
    "Feature engineering refers to developing some new features by using existing features. Sometimes there is a very subtle mathematical relation between some features which if explored properly then the new features can be developed using those mathematical operations. At those times developing new features and using them help us to gain deeper insights into the data as well as if the features derived are significant enough helps to improve the model’s performance a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec4bac-37d8-4ec7-bbb1-4e4f8ccf4f69",
   "metadata": {},
   "source": [
    "# Q6. Whether decision tree or random forest is more robust to the outliers.\n",
    "Decision trees and random forests are both relatively robust to outliers. A random forest model is an ensemble of multiple decision trees so, the output of a random forest model is an aggregate of multiple decision trees.\n",
    "\n",
    "So, when we average the results the chances of overfitting get reduced. Hence we can say that the random forest models are more robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a697e-8969-4920-88b5-f140a5bcc137",
   "metadata": {},
   "source": [
    "# Q7. What is Principal Component Analysis?\n",
    "PCA(Principal Component Analysis) is an unsupervised machine learning dimensionality reduction technique in which we trade off some information or patterns of the data at the cost of reducing its size significantly. In this algorithm, we try to preserve the variance of the original dataset up to a great extent let’s say 95%. For very high dimensional data sometimes even at the loss of 1% of the variance, we can reduce the data size significantly.\n",
    "\n",
    "By using this algorithm we can perform image compression, visualize high-dimensional data as well as make data visualization easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cd39d-4434-454c-a042-265f24630d6c",
   "metadata": {},
   "source": [
    "# Q8. What is Overfitting in Machine Learning and how can it be avoided?\n",
    "Overfitting happens when the model learns patterns as well as the noises present in the data this leads to high performance on the training data but very low performance for data that the model has not seen earlier. To avoid overfitting there are multiple methods that we can use:\n",
    "\n",
    "- Early stopping of the model’s training in case of validation training stops increasing but the training keeps going on.\n",
    "- Using regularization methods like L1 or L2 regularization which is used to penalize the model’s weights to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b09bb-ceb2-487d-b958-ee3d14a160fe",
   "metadata": {},
   "source": [
    "# Q9. What is the bias-variance tradeoff?\n",
    "\n",
    "- Bias refers to the difference between the actual values and the predicted values by the model. Low bias means the model has learned the pattern in the data and high bias means the model is unable to learn the patterns present in the data i.e the underfitting.\n",
    "- Variance refers to the change in accuracy of the model’s prediction on which the model has not been trained. Low variance is a good case but high variance means that the performance of the training data and the validation data vary a lot.\n",
    "\n",
    "If the bias is too low but the variance is too high then that case is known as overfitting. So, finding a balance between these two situations is known as the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48958262-89dc-42e6-a94f-df4bca76d73b",
   "metadata": {},
   "source": [
    "# Q10. What is the difference between L1 and L2 regularization? What is their significance?\n",
    "L1 regularization: In L1 regularization also known as Lasso regularization in which we add the sum of absolute values of the weights of the model in the loss function. In L1 regularization weights for those features which are not at all important are penalized to zero so, in turn, we obtain feature selection by using the L1 regularization technique.\n",
    "\n",
    "L2 regularization: In L2 regularization also known as Ridge regularization in which we add the square of the weights to the loss function. In both of these regularization methods, weights are penalized but there is a subtle difference between the objective they help to achieve. \n",
    "\n",
    "In L2 regularization the weights are not penalized to 0 but they are near zero for irrelevant features. It is often used to prevent overfitting by shrinking the weights towards zero, especially when there are many features and the data is noisy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
