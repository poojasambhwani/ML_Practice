{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization\n",
    "\n",
    "Standardization transforms data to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26346568  1.27872403  1.01194625]\n",
      " [ 0.08151391 -0.11624764  0.34958143]\n",
      " [ 1.18195176 -1.16247639 -1.36152768]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Raw data\n",
    "data = np.array([[1.2, 3.5, 5.8], [2.3, 3.1, 4.6], [3.2, 2.8, 1.5]])\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "print(standardized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Normalization scales data to fit within a specific range, typically [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         1.        ]\n",
      " [0.55       0.42857143 0.72093023]\n",
      " [1.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Imputer\n",
    "\n",
    "It is a data imputation technique that fills missing values with a defined constant, mean, or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2  3.5  3.05]\n",
      " [1.75 2.1  4.6 ]\n",
      " [2.3  2.8  1.5 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Data with missing values\n",
    "data_with_nan = np.array([[1.2, 3.5, np.nan], [np.nan, 2.1, 4.6], [2.3, np.nan, 1.5]])\n",
    "\n",
    "# Simple Imputer (mean strategy)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data = imputer.fit_transform(data_with_nan)\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Imputer\n",
    "\n",
    "It is a data imputation technique that fills missing values based on the nearest neighbors in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2  3.5  3.05]\n",
      " [1.75 2.1  4.6 ]\n",
      " [2.3  2.8  1.5 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_knn_data = knn_imputer.fit_transform(data_with_nan)\n",
    "print(imputed_knn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Converts categorical variables into numerical form (Label Encoding, Ordinal Encoding or One-Hot Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Example categorical labels\n",
    "labels = ['cat', 'dog', 'fish', 'dog']\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Example ordinal data\n",
    "categories = np.array([['low'], ['medium'], ['high'], ['medium']])\n",
    "\n",
    "# Ordinal encoding (Assumes 'low' < 'medium' < 'high')\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\n",
    "encoded_data = ordinal_encoder.fit_transform(categories)\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Categorical data\n",
    "categories = np.array([['red'], ['blue'], ['green'], ['blue']])\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(categories)\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers apply functions to convert data during preprocessing (e.g., log transformation).\n",
    "\n",
    "Function Transformer: A FunctionTransformer applies any custom function (like log transformation, square root, etc.) to the data. \n",
    "\n",
    "Column Transformer: A ColumnTransformer allows different preprocessing steps (like scaling, encoding, etc.) to be applied to specific columns in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78845736 1.5040774 ]\n",
      " [1.19392247 1.41098697]\n",
      " [1.43508453 1.33500107]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Raw data\n",
    "data = np.array([[1.2, 3.5], [2.3, 3.1], [3.2, 2.8]])\n",
    "\n",
    "# Applying log transformation using FunctionTransformer\n",
    "log_transformer = FunctionTransformer(np.log1p)  # log1p is log(1+x) to avoid log(0)\n",
    "transformed_data = log_transformer.fit_transform(data)\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26346568  1.27872403  0.          0.          1.        ]\n",
      " [ 0.08151391 -0.11624764  1.          0.          0.        ]\n",
      " [ 1.18195176 -1.16247639  0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset (2 numerical columns and 1 categorical column)\n",
    "data = np.array([[1.2, 3.5, 'red'], [2.3, 3.1, 'blue'], [3.2, 2.8, 'green']])\n",
    "\n",
    "# ColumnTransformer: apply standardization to numerical columns and OneHotEncoder to the categorical column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [0, 1]),  # Standardize columns 0 and 1\n",
    "        ('cat', OneHotEncoder(), [2])       # One-hot encode column 2\n",
    "    ]\n",
    ")\n",
    "\n",
    "transformed_data = preprocessor.fit_transform(data)\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Pipelines streamline multiple data preprocessing steps (e.g., imputation, transformation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26346568  1.22474487  0.          0.          1.        ]\n",
      " [ 0.08151391  0.          1.          0.          0.        ]\n",
      " [ 1.18195176 -1.22474487  0.          1.          0.        ]]\n",
      "[0 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Example dataset with numerical and categorical columns\n",
    "data = np.array([[1.2, 3.5, 'red'], \n",
    "                 [2.3, np.nan, 'blue'], \n",
    "                 [3.2, 2.8, 'green']])\n",
    "labels = np.array([0, 1, 0])  # Target labels for classification\n",
    "\n",
    "# Define a ColumnTransformer for mixed data types\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),  # Impute missing numerical values\n",
    "        ('scaler', StandardScaler())  # Standardize numerical features\n",
    "    ]), [0, 1]),  # Apply to first two columns (numerical)\n",
    "    \n",
    "    ('cat', OneHotEncoder(), [2])  # One-hot encode the categorical column (third column)\n",
    "])\n",
    "\n",
    "# Create a full pipeline with preprocessing and a classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Preprocessing (imputation, scaling, encoding)\n",
    "    ('classifier', RandomForestClassifier())  # Classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(data, labels)\n",
    "\n",
    "# To transform the data using the preprocessor part of the pipeline\n",
    "pipeline_data = pipeline.named_steps['preprocessor'].transform(data)\n",
    "print(pipeline_data)\n",
    "\n",
    "# If you want to make predictions, you can use:\n",
    "predictions = pipeline.predict(data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Underfitting\n",
    "\n",
    "Overfitting: Model performs well on training data but poorly on unseen data because it captures noise.\n",
    "\n",
    "Underfitting: Model is too simple, fails to capture patterns in training data and performs poorly on both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9595085352689678\n",
      "Test Score: 0.9534093919138027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Example of overfitting using Linear Regression on small dataset\n",
    "X = np.random.rand(100, 1) * 10  # Feature\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2  # Target with noise\n",
    "\n",
    "# Overfitting model with too many features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Train Score:\", train_score)\n",
    "print(\"Test Score:\", test_score)  # Expect large difference if overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.992575959005177\n",
      "Test Score: 0.9904702646213497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple dataset\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.randn(100) * 0.5  # Simple linear relationship with noise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Create a very simple model (underfitting)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Score: {train_score}\")\n",
    "print(f\"Test Score: {test_score}\")  # Expect both to be low due to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization (Lasso)\n",
    "\n",
    "It is a regualization technique that shrinks less important feature coefficients to 0, effectively selecting features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Coefficients: [2.94780947]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Lasso (L1 regularization)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"Lasso Coefficients:\", lasso.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization (Ridge)\n",
    "\n",
    "It is a regualization technique that reduces coefficients to prevent overfitting but keeps all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Coefficients: [2.95792088]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge (L2 regularization)\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net Regularization\n",
    "\n",
    "Elastic Net combines both L1 (Lasso) and L2 (Ridge) regularization techniques. It encourages both feature selection (like L1) and coefficient shrinkage (like L2). This is useful when dealing with data that has high multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net Coefficients: [2.93758142]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Elastic Net model (combining L1 and L2 regularization)\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances L1 and L2\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "print(\"Elastic Net Coefficients:\", elastic_net.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers\n",
    "\n",
    "IQR (Interquartile Range) Method: Identifies outliers as data points falling below Q1 - 1.5IQR or above Q3 + 1.5IQR.\n",
    "    \n",
    "Z-Score Method: Outliers are data points more than 3 standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers: [100]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Outlier detection using IQR\n",
    "data = np.array([10, 12, 14, 15, 16, 17, 19, 22, 100])  # 100 is an outlier\n",
    "Q1, Q3 = np.percentile(data, [25, 75])\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "print(\"Outliers:\", outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers based on Z-scores: []\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Z-score method for outlier detection\n",
    "z_scores = zscore(data)\n",
    "outliers_z = data[np.abs(z_scores) > 3]\n",
    "print(\"Outliers based on Z-scores:\", outliers_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
