{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b978f86c-4384-4b70-a1f6-ab0260ae70dc",
   "metadata": {},
   "source": [
    "# Machine Learning Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbca59-bfa5-4d6c-842f-16aa6873ad73",
   "metadata": {},
   "source": [
    "# Q1. Why do we take the harmonic mean of precision and recall when finding the F1-score and not simply the mean of the two metrics?\n",
    "\n",
    "A. The F1-score, the harmonic mean of precision and recall, balances the trade-off between precision and recall. The harmonic mean penalizes extreme values more than the arithmetic mean. This is crucial for cases where one of the metrics is significantly lower than the other. In classification tasks, precision and recall may have an inverse relationship; therefore, the harmonic mean ensures that the F1-score gives equal weight to precision and recall, providing a more balanced evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81376abc-697f-48fa-9d34-ac0cf7a379b8",
   "metadata": {},
   "source": [
    "# Q2. Why does Logistic regression have regression in its name even if it is used specifically for Classification?\n",
    "\n",
    "A. Logistic regression doesn’t directly classify but uses a linear model to estimate the probability of an event (0-1). We then choose a threshold (like 50%) to convert this to categories like ‘yes’ or ‘no’. So, despite the ‘regression’ in its name, it ultimately tells us which class something belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e0fe8-5bd2-4f46-8b48-30126c184a16",
   "metadata": {},
   "source": [
    "# Q3. If you do not know whether your data is scaled, and you have to work on the classification problem without looking at the data, then out of Random Forest and Logistic Regression, which technique will you use and why?\n",
    "\n",
    "A. In this scenario, Random Forest would be a more suitable choice. Logistic Regression is sensitive to the scale of input features, and unscaled features can affect its performance. On the other hand, Random Forest is less impacted by feature scaling due to its ensemble nature. Random Forest builds decision trees independently, and the scaling of features doesn’t influence the splitting decisions across trees. Therefore, when dealing with unscaled data and limited insights, Random Forest would likely yield more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975c345-934b-4882-8a2a-f02599f14505",
   "metadata": {},
   "source": [
    "# Q4. In a binary classification problem aimed at identifying cancer in individuals, if you had to prioritize one performance metric over the other, considering you don’t want to risk any person’s life, which metric would you be more willing to compromise on, Precision or Recall, and why?\n",
    "\n",
    "A. In identifying cancer, recall (sensitivity) is more critical than precision. Maximizing recall ensures that the model correctly identifies as many positive cases (cancer instances) as possible, reducing the chances of false negatives (missed cases). False negatives in cancer identification could have severe consequences. While precision is important to minimize false positives, prioritizing recall helps ensure a higher sensitivity to actual positive cases in the medical domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ef75e-7e3e-4d4b-b99c-8a7b34a17c12",
   "metadata": {},
   "source": [
    "# Q5. What is the significance of P-value when building a Machine Learning model?\n",
    "A. P-values are used in traditional statistics to determine the significance of a particular effect or parameter. P-value can be used to find the more relevant features in making predictions. The closer the value to 0, the more relevant the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4247dc-7563-449e-8626-db9f6f38722f",
   "metadata": {},
   "source": [
    "# Q6. How does skewness in the distribution of a dataset affect the performance or behavior of machine learning models?\n",
    "A. Skewness in the distribution of a dataset can significantly impact the performance and behavior of machine learning models. Here’s an explanation of its effects and how to handle skewed data:\n",
    "\n",
    "Effects of Skewed Data on Machine Learning Models:\n",
    "\n",
    "1. Bias in Model Performance: Skewed data can introduce bias in model training, especially with algorithms sensitive to class distribution. Models might be biased towards the majority class, leading to poor predictions for the minority class in classification tasks.\n",
    "2. Impact on Algorithms: Skewed data can affect the decision boundaries learned by models. For instance, in logistic regression or SVMs, the decision boundary might be biased towards the dominant class when one class dominates the other.\n",
    "3. Prediction Errors: Skewed data can result in inflated accuracy metrics. Models might achieve high accuracy by simply predicting the majority class yet fail to detect patterns in the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c374f55-02e0-431c-95a0-b8cdddb8235d",
   "metadata": {},
   "source": [
    "# Q7. How would you detect outliers in a dataset?\n",
    "A. Outliers can be detected using various methods, including:\n",
    "\n",
    "1. Z-Score: Identify data points with a Z-score beyond a certain threshold.\n",
    "2. IQR (Interquartile Range): Flag data points outside the 1.5 times the IQR range.\n",
    "3. Visualization: Plotting box plots, histograms, or scatter plots can reveal data points significantly deviating from the norm.\n",
    "4. Machine Learning Models: Outliers may be detected using models trained to identify anomalies, like one-class SVMs or Isolation Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b715e-6b81-4cdb-9f34-8d503ad58229",
   "metadata": {},
   "source": [
    "# Q8. Explain the Bias-Variance Tradeoff in Machine Learning. How does it impact model performance?\n",
    "A. The bias-variance tradeoff refers to the delicate balance between the error introduced by bias and variance in machine learning models. A model with high bias oversimplifies the underlying patterns, leading to poor performance in training and unseen data. Conversely, a model with high variance captures noise in the training data and fails to generalize to new data.\n",
    "\n",
    "Balancing bias and variance is crucial. Reducing bias often increases variance and vice versa. Optimal model performance is finding the right tradeoff to achieve low training and test data error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91286934-c402-409a-bd40-6101d1ed1d4a",
   "metadata": {},
   "source": [
    "# Q9. Describe the working principle behind Support Vector Machines (SVMs) and their kernel trick. When would you choose SVMs over other algorithms?\n",
    "A. SVMs aim to find the optimal hyperplane that separates classes with the maximum margin. The kernel trick allows SVMs to operate in a high-dimensional space, transforming non-linearly separable data into a linearly separable one.\n",
    "\n",
    "Choose SVMs when:\n",
    "\n",
    "1. Dealing with high-dimensional data.\n",
    "2. Aiming for a clear margin of separation between classes.\n",
    "3. Handling non-linear relationships with the kernel trick.\n",
    "4. In scenarios where interpretability is less critical compared to predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae3a82-8e0c-466b-a36f-61a54cdcc8b2",
   "metadata": {},
   "source": [
    "# Q10. Explain the difference between lasso and ridge regularization.\n",
    "A. Both lasso and ridge regularization are techniques to prevent overfitting by adding a penalty term to the loss function. The key difference lies in the type of penalty:\n",
    "\n",
    "1. Lasso (L1 regularization): Adds the absolute values of coefficients to the loss function, encouraging sparse feature selection. It tends to drive some coefficients to exactly zero.\n",
    "2. Ridge (L2 regularization): Adds the squared values of coefficients to the loss function. It discourages large coefficients but rarely leads to sparsity.\n",
    "3. Choose lasso when feature selection is crucial and(overfitting)  ridge when all features contribute meaningfully to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce96e53-14b2-4ba1-8a98-71ec41187b4d",
   "metadata": {},
   "source": [
    "# Q11. Explain the concept of self-supervised learning in machine learning.\n",
    "A. Self-supervised learning is a paradigm where models generate their labels from the existing data. It leverages the inherent structure or relationships within the data to create supervision signals without human-provided labels. Common self-supervised tasks include predicting missing parts of an image, filling in masked words in a sentence, or generating a relevant part of a video sequence. This approach is valuable when labeled data is relatively inexpensive to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabdaef-4cbb-4b76-92c4-10c4c7a2af56",
   "metadata": {},
   "source": [
    "# Q12. Explain the concept of Bayesian optimization in hyperparameter tuning. How does it differ from grid search or random search methods?\n",
    "A. Bayesian optimization is an iterative model-based optimization technique that uses probabilistic models to guide the search for optimal hyperparameters. Unlike grid search or random search, Bayesian optimization considers the information gained from previous iterations, directing the search towards promising regions of the hyperparameter space. This approach is more efficient, requiring fewer evaluations, making it suitable for complex and computationally expensive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b921f-8f8a-4423-962a-1882f9a10450",
   "metadata": {},
   "source": [
    "# Q13. Explain the difference between semi-supervised and self-supervised learning.\n",
    "Semi-Supervised Learning: Involves training a model with both labeled and unlabeled data. The model learns from the labeled examples while leveraging the structure or relationships within the unlabeled data to improve generalization.\n",
    "Self-Supervised Learning: The model generates its labels from the existing data without external annotations. The learning task is designed so that the model predicts certain parts or features of the data, creating its supervision signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ab739-9835-4392-8dd7-0b91135f4c06",
   "metadata": {},
   "source": [
    "# Q14. What are the advantages of using Random Forest over a single decision tree?\n",
    "Reduced Overfitting: Random Forest mitigates overfitting by training multiple trees on different subsets of the data and averaging their predictions, providing a more generalized model.\n",
    "Improved Accuracy: The ensemble nature of Random Forest often results in higher accuracy compared to a single decision tree, especially for complex datasets.\n",
    "Feature Importance: Random Forest measures feature importance, helping identify the most influential variables in the prediction process.\n",
    "Robustness to Outliers: Random Forest is less sensitive to outliers due to the averaging effect of multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2138d12-6190-4711-a5c8-ea87f30c8058",
   "metadata": {},
   "source": [
    "# Q15. Differentiate between feature selection and feature extraction.\n",
    "Feature Selection: Feature selection involves choosing a subset of the most relevant features from the original set. The goal is to eliminate irrelevant or redundant features, reduce dimensionality, and improve model interpretability and efficiency. Methods include filter methods (based on statistical metrics), wrapper methods (using models to evaluate feature subsets), and embedded methods (incorporated into the model training process).\n",
    "\n",
    "Feature Extraction: Feature extraction transforms the original features into a new set of features, often of lower dimensionality. Techniques like Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) project data into a new space, capturing essential information while discarding less relevant details. Feature extraction is particularly useful when dealing with high-dimensional data or when feature interpretation is less critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e07a3-875b-4e00-9af1-2a4e08d05c3e",
   "metadata": {},
   "source": [
    "# Q16. How can cross-validation help in improving the performance of a model?\n",
    "A. Cross-validation helps assess and improve model performance by evaluating how well a model generalizes to new data. It involves splitting the dataset into multiple subsets (folds), training the model on different folds, and validating it on the remaining folds. This process is repeated multiple times, and the average performance is computed. Cross-validation provides a more robust estimate of a model’s performance, helps identify overfitting, and guides hyperparameter tuning for better generalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e5def-eef0-4c2a-9ecc-6abc4957cd11",
   "metadata": {},
   "source": [
    "# Q17. Differentiate between feature scaling and feature normalization. What are their primary goals and distinctions?\n",
    "Feature Scaling: Feature scaling is a general term that refers to standardizing or transforming the scale of features to a consistent range. It prevents features with larger scales from dominating those with smaller scales during model training. Scaling methods include Min-Max Scaling, Z-score (standardization), and Robust Scaling.\n",
    "    \n",
    "Feature Normalization: Feature normalization involves transforming features to a standard normal distribution with a mean of 0 and a standard deviation of 1 (Z-score normalization). It is a type of feature scaling that emphasizes achieving a specific distribution for the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c2dfb0-34af-4646-8417-d09e2938f100",
   "metadata": {},
   "source": [
    "# Q18. Explain choosing an appropriate scaling/normalization method for a specific machine-learning task. What factors should be considered?\n",
    "A. Choosing a scaling/normalization method depends on the characteristics of the data and the requirements of the machine-learning task:\n",
    "\n",
    "1. Min-Max Scaling: Suitable for algorithms sensitive to the scale of features (e.g., neural networks). Works well when data follows a uniform distribution.\n",
    "2. Z-score Normalization (Standardization): Suitable for algorithms assuming features are normally distributed. Resistant to outliers.\n",
    "3. Robust Scaling: Suitable when the dataset contains outliers. It scales features based on the interquartile range.\n",
    "4. Consider the characteristics of the algorithm, the distribution of features, and the presence of outliers when selecting a method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a77fc9-919e-4874-ae01-54e63405f7e3",
   "metadata": {},
   "source": [
    "# Q19. Compare and contrast z-scores with other standardization methods like min-max scaling.\n",
    "1. Z-Score (Standardization): Scales feature a mean of 0 and a standard deviation of 1. Suitable for normal distribution and is less sensitive to outliers.\n",
    "2. Min-Max Scaling: Often, features are transformed to a specific range [0, 1]. Preserves the original distribution and is sensitive to outliers.\n",
    "\n",
    "Both methods standardize features, but z-scores are suitable for normal distributions and robust to outliers. At the same time, min-max scaling is simple and applicable when preserving the original distribution is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b62424-818a-4484-a945-87dc96e5bde0",
   "metadata": {},
   "source": [
    "# Q20. What Is a False Positive and False Negative, and How Are They Significant?\n",
    "1. False Positive (FP): In binary classification, a false positive occurs when the model predicts the positive class incorrectly. It means the model incorrectly identifies an instance as belonging to the positive class when it belongs to the negative class.\n",
    "2. False Negative (FN): A false negative occurs when the model predicts the negative class incorrectly. It means the model fails to identify an instance that belongs to the positive class.\n",
    "\n",
    "Significance:\n",
    "\n",
    "1. False Positives: In applications like medical diagnosis, a false positive can lead to unnecessary treatments or interventions, causing patient distress and additional costs.\n",
    "2. False Negatives: In critical scenarios like disease detection, a false negative may result in undetected issues, delaying necessary actions and potentially causing harm.\n",
    "\n",
    "The significance depends on the specific context of the problem and the associated costs or consequences of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425bfe83-1203-43dc-afec-fbf8054abf9b",
   "metadata": {},
   "source": [
    "# Q21. What is PCA in Machine Learning, and can it be used for selecting features?\n",
    "PCA (Principal Component Analysis): PCA is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space while retaining as much variance as possible. It identifies principal components, which are linear combinations of the original features.\n",
    "    \n",
    "Feature Selection with PCA: While PCA is primarily used for dimensionality reduction, it indirectly performs feature selection by highlighting the most informative components. However, there may be better choices for feature selection when the interpretability of individual features is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa06bcf-8d15-4de4-a371-961b88de4823",
   "metadata": {},
   "source": [
    "# Q22. The model you have trained has a high bias and low variance. How would you deal with it?\n",
    "Addressing a model with high bias and low variance involves:\n",
    "\n",
    "1. Increase Model Complexity: Choose a more complex model that can better capture the underlying patterns in the data. For example, move from a linear model to a non-linear one.\n",
    "2. Feature Engineering: Introduce additional relevant features the model may be missing to improve its learning ability.\n",
    "3. Reduce Regularization: If the model has regularization parameters, consider reducing them to allow it to fit the training data more closely.\n",
    "4. Ensemble Methods: Utilize ensemble methods, combining predictions from multiple models, to improve overall performance.\n",
    "5. Hyperparameter Tuning: Experiment with hyperparameter tuning to find the optimal settings for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59937bf-6c6d-4e28-aa5a-5786dc070030",
   "metadata": {},
   "source": [
    "# Q23. What is the interpretation of a ROC area under the curve?\n",
    "A. The Receiver Operating Characteristic (ROC) curve is a graphical representation of a binary classification model’s performance across different discrimination thresholds. The Area Under the Curve (AUC) measures the model’s overall performance. The interpretation of AUC is as follows:\n",
    "\n",
    "1. AUC = 1: Perfect classifier with no false positives and false negatives.\n",
    "2. AUC = 0.5: The model performs no better than random chance.\n",
    "3. AUC > 0.5: The model performs better than random chance.\n",
    "    \n",
    "A higher AUC indicates better discrimination ability, with values closer to 1 representing superior performance. The ROC AUC is handy for evaluating models with class imbalance or considering different operating points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73855c5-57cd-4fff-826f-7ae09b03b22e",
   "metadata": {},
   "source": [
    "# Q24: What is naive in the Naive Bayes algorithm?\n",
    "Naive Bayes is a popular machine learning algorithm that works on the Bayes theorem. Naive means innocent (simple). It is a supervised machine learning algorithm where you have many independent columns and one output column. Also, naive input columns are independent, and in normal, the data have some relationship, but the naive Bayes does not assume this. For its work, there must be no relationship between input columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78eebe3-c94c-4ac6-ac06-c883a0e65cfb",
   "metadata": {},
   "source": [
    "# Q25: Can you give an example of Where the Median is better to measure than the mean?\n",
    "You should know about mean and median if you have read basic descriptive statistics. The mean is the average of all the observations (total sum divided by a total number of observations). The Median is the center number obtained after sorting all the observations. Both measures show the central tendency of the data. So when we have outliers in data, using the mean in this condition is not recommended.\n",
    "\n",
    "For example, we have a dataset of several students with annual packages. All the students got the package between 3 to 6 LPA, but 2-3 students have packages as 25LPA, and 38LPA, and when we are asked to give an average class package, then the mean will be a huge number which is wrong in this case. So better to use Median in such types of cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de233a01-28f1-474e-bc04-b95942f86199",
   "metadata": {},
   "source": [
    "# Q26: In what scenario would you prefer a decision tree over a random forest?\n",
    "The question can be asked to check your practical knowledge of machine learning. A decision tree is a simple algorithm that works on an ID3 or CART basis. And a collection of multiple decision trees is a random forest. If we talk more practically than in most datasets, the random forest performs better than the decision tree. But there are some points where a decision tree is more useful than a random forest.\n",
    "\n",
    "1. Explainability – Always, you do not need performance, but you also need to explain the work and solution. And when you solve any problem with a decision tree, you get a proper tree-like structure that explains the complete tree-building process, and you can easily discuss the model’s working with the client and manager.\n",
    "2. Computation power – If you have less computation power, you can go with a decision tree.\n",
    "3. Useful Features – This is a practical use case when you have some handy features that you want should use on priority then, a decision tree is helpful because, in a random forest, features are selected at random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d3305-30f1-4825-ab25-8de11a95a19b",
   "metadata": {},
   "source": [
    "# Q27: Why is logistic regression called regression, not logistic classification?\n",
    "The logistic regression works closely with the linear regression model. The only difference is you use the sigmoid function in output and calculate probability, and using a threshold gives the result as 0 or 1. The regression is so called because it calculates a constant value probability. When we calculate the continuous value, it is called the regression algorithm, so the logistic algorithm is called the regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac22d99-6c75-49a4-b212-6452d30b2dfb",
   "metadata": {},
   "source": [
    "# Q28: What is the difference between structured and unstructured data?\n",
    "The data you receive in machine learning is of two types structured and unstructured.\n",
    "\n",
    "Structured – Data in the tabular form is known as structured data. If we say tabular, the data collects many rows and columns. Data in excel sheet format are structured data. In structured data, you will always find text inside the columns. Searching in structured data is simple. Traditional ML algorithms are easily applicable to structured data. Structured data is mainly used in the Analytics domain.\n",
    "\n",
    "Unstructured Data – Unorganized data contains different types of files like images, audio, video, GIFs, text files, etc. Search becomes difficult in unstructured data. Here mainly deep learning techniques are used. Unstructured data is used in NLP, text-mining, and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f202952-d3c6-4b09-ba2b-61b800f4944c",
   "metadata": {},
   "source": [
    "# Q29: What are Correlation and covariance?\n",
    "Correlation describes the relationship between two strongly positive or negative correlated variables. It is used to figure out the quantitative relationship between two variables. Examples like income and expenditure, demand and supply, etc.\n",
    "\n",
    "Covariance is a simple way to calculate the correlation between two variables. The problem with covariance is that they are hard to compare without normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00c8d8-8729-497d-bb6c-ac43e5e98361",
   "metadata": {},
   "source": [
    "# Q30: Explain the Bias-variance trade-off.\n",
    "Bias and variance are both a type of errors that ML algorithms reflect. Bias occurs due to the simplistic assumption of the machine learning algorithm. When the model does not perform well on training data, then the model is reflected as high bias or the condition of underfitting occurs.\n",
    "\n",
    "Variance is an error that occurs due to the complexity of the algorithm. When the algorithm cannot predict approximate results on new data or tries to overfit the model, we have a high variance combination.\n",
    "\n",
    "We need to trade between bias and variance to reduce the error optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a051dc4-eead-4fa3-b3a8-f842fd4e474e",
   "metadata": {},
   "source": [
    "# Q31. What is overfitting? How do you prevent it?\n",
    "Ans. Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and outliers. This results in excellent performance on the training data but poor generalization to new, unseen data. Here are a few strategies to prevent overfitting:\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to ensure the model performs well on different subsets of the data.\n",
    "Regularization: Add a penalty for larger coefficients (L1 or L2 regularization) to simplify the model.\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0)\n",
    "Pruning (for decision trees): Trim the branches of a tree that have little importance.\n",
    "Early Stopping: Stop training when the model performance on a validation set starts to degrade.\n",
    "Dropout (for neural networks): Randomly drop neurons during training to prevent co-adaptation.\n",
    "from tensorflow.keras.layers import Dropout\n",
    "model.add(Dropout(0.5))\n",
    "More Data: Increasing the size of the training dataset can help the model generalize better.\n",
    "Preventing overfitting is crucial for building robust models that perform well on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc65a4-4bb3-4b0f-b128-fc7df6bb173e",
   "metadata": {},
   "source": [
    "# Q32. Explain the difference between supervised and unsupervised learning. Give an example.\n",
    "Ans. Supervised and unsupervised learning are two fundamental types of machine learning.\n",
    "\n",
    "Supervised Learning: In this approach, the model is trained on labeled data, meaning that each training example comes with an associated output label. The goal is to learn a mapping from inputs to outputs. Common tasks include classification and regression.\n",
    "\n",
    "Example: Supervised learning with a classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "Unsupervised Learning: In this approach, the model is trained on data without labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data. Common tasks include clustering and dimensionality reduction.\n",
    "\n",
    "Example: Unsupervised learning with a clustering algorithm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(X_train)\n",
    "\n",
    "The main difference lies in the presence or absence of labeled outputs during training. Supervised learning is used when the goal is prediction, while unsupervised learning is used for discovering patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e547277-7f96-4b70-a731-e7986c0e0477",
   "metadata": {},
   "source": [
    "# Q33. What is the difference between classification and regression?\n",
    "Ans. Classification and regression are both types of supervised learning tasks, but they serve different purposes.\n",
    "\n",
    "Classification: This involves predicting a categorical outcome. The goal is to assign inputs to one of a set of predefined classes.\n",
    "\n",
    "Example: Classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "Regression: This involves predicting a continuous outcome. The goal is to predict a numeric value based on input features.\n",
    "\n",
    "Example: Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "In summary, classification predicts discrete labels, while regression predicts continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2ef9a-c0f7-4afb-bcf3-6461a4a09748",
   "metadata": {},
   "source": [
    "# Q34. Write a Python script to perform Principal Component Analysis (PCA) on a dataset and plot the first two principal components.\n",
    "Ans. I used an example DataFrame df with three features. Performed PCA to reduce the dimensionality to 2 components using PCA from sklearn and plotted the first two principal components using matplotlib. Here’s how you can do it:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Example DataFrame\n",
    "\n",
    "df = pd.DataFrame({\n",
    "'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "'feature2': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "'feature3': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] })\n",
    "\n",
    "X = df[['feature1', 'feature2', 'feature3']]\n",
    "\n",
    "Step 1: Apply PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X)\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "Step 2: Plot the first two principal components\n",
    "\n",
    "plt.scatter(principal_df['PC1'], principal_df['PC2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33abec42-7839-4528-952e-cbfab9edf163",
   "metadata": {},
   "source": [
    "# Q35. How do you evaluate a machine learning model?\n",
    "Ans. Evaluating a machine learning model involves several metrics and techniques to ensure its performance. Here are some common methods:\n",
    "\n",
    "Train-Test Split: Divide the dataset into a training set and a test set to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Cross-Validation: Use k-fold cross-validation to assess the model’s performance on different subsets of the data.\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "Confusion Matrix: For classification problems, a confusion matrix helps visualize the performance by showing true vs. predicted values.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ROC-AUC Curve: For binary classification, the ROC-AUC curve helps evaluate the model’s ability to distinguish between classes.\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE): For regression problems, these metrics help quantify the prediction errors.\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "Evaluating a model comprehensively ensures that it performs well not just on training data but also on new, unseen data, making it robust and reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
